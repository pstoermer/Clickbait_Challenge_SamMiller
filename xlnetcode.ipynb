{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\esser\\.virtualenvs\\anlp-Bd8LEaH2\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from transformers import XLNetModel, XLNetTokenizer, XLNetForSequenceClassification\n",
    "from transformers import AdamW\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src import utils\n",
    "\n",
    "# Import label encoder\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = utils.load_data(\"data/train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentence and label lists\n",
    "sentences = df.postText.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [sentence[0] + \" [SEP] [CLS]\" for sentence in sentences]\n",
    "labels = [i[0] for i in df.tags.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_encoder object knows how to understand word labels.\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 2 ... 0 2 2]\n",
      "['passage' 'phrase' 'phrase' ... 'multi' 'phrase' 'phrase']\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "written_labels = label_encoder.inverse_transform(labels)\n",
    "print(written_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding_array = {1: \"passage\",\n",
    "2 : \"phrase\",\n",
    "0 : \"multi\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['▁we', 's', '▁', 'wel', 'ker', '▁wanted', '▁dinner', '▁with', '▁to', 'm', '▁bra', 'dy', ',', '▁but', '▁patriot', 's', '▁', 'q', 'b', '▁had', '▁better', '▁idea', '▁[', 's', 'ep', ']', '▁[', 'cl', 's', ']']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
    "MAX_LEN = 128\n",
    "# Use the XLNet tokenizer to convert the tokens to their index numbers in the XLNet vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask)\n",
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2880, 128)\n",
      "(320, 128)\n",
      "(2880,)\n",
      "(320,)\n"
     ]
    }
   ],
   "source": [
    "print(train_inputs.shape)\n",
    "print(validation_inputs.shape)\n",
    "print(train_labels.shape)\n",
    "print(validation_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning with XLNet, the authors recommend a batch size of 32, 48, or 128. We will use 32 here to avoid memory issues.\n",
    "batch_size = 32\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load XLNEtForSequenceClassification, the pretrained XLNet model with a single linear classification layer on top. \n",
    "\n",
    "model = XLNetForSequenceClassification.from_pretrained(\"xlnet-large-cased\", num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\esser\\.virtualenvs\\anlp-Bd8LEaH2\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# This variable contains all of the hyperparemeter information our training loop needs\n",
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                     lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.0571460670895048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  17%|█▋        | 1/6 [1:08:45<5:43:46, 4125.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.440625\n",
      "Train loss: 1.0537956655025482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  33%|███▎      | 2/6 [2:16:54<4:33:35, 4103.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.4375\n",
      "Train loss: 1.0432039505905575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 3/6 [3:26:10<3:26:23, 4127.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.53125\n",
      "Train loss: 0.8903786096307966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  67%|██████▋   | 4/6 [5:09:15<2:44:39, 4939.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.68125\n",
      "Train loss: 0.7584818604919645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  83%|████████▎ | 5/6 [6:44:13<1:26:53, 5213.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.68125\n",
      "Train loss: 0.6399713009595871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 6/6 [8:16:16<00:00, 4962.72s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.659375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 6\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "  \n",
    "  \n",
    "  # Training\n",
    "  \n",
    "  # Set our model to training mode (as opposed to evaluation mode)\n",
    "  model.train()\n",
    "  \n",
    "  # Tracking variables\n",
    "  tr_loss = 0\n",
    "  nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "  # Train the data for one epoch\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # Clear out the gradients (by default they accumulate)\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "    loss = outputs[0]\n",
    "    logits = outputs[1]\n",
    "    train_loss_set.append(loss.item())    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    # Update parameters and take a step using the computed gradient\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    # Update tracking variables\n",
    "    tr_loss += loss.item()\n",
    "    nb_tr_examples += b_input_ids.size(0)\n",
    "    nb_tr_steps += 1\n",
    "\n",
    "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    \n",
    "    \n",
    "  # Validation\n",
    "\n",
    "  # Put model in evaluation mode to evaluate loss on the validation set\n",
    "  model.eval()\n",
    "\n",
    "  # Tracking variables \n",
    "  eval_loss, eval_accuracy = 0, 0\n",
    "  nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "  # Evaluate data for one epoch\n",
    "  for batch in validation_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "      logits = output[0]\n",
    "    \n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    \n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"xltask1_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/EstrixDS/XLNet_SemEval_Task1/commit/51e56bb7e186aefca2c870907531d01b30525556', commit_message='Upload XLNetForSequenceClassification', commit_description='', oid='51e56bb7e186aefca2c870907531d01b30525556', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"EstrixDS/XLNet_SemEval_Task1\",use_auth_token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passage\n",
      "multi\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame([\n",
    "    {\"uuid\": \"b2303ab7-f978-4576-b563-899f73397ed5\", \"postText\": [\"Guess who hasn't seen the Star Wars: The Force awakens trailer?\"], \"targetParagraphs\": [\"On YouTube alone, the teaser trailer for Star Wars: The Force Awakens has more than 12 million views, but you can count one person out of that number: George Lucas, the man who created the Star Wars universe.\", \"Page Six asked Lucas for his thoughts on the 88-second glimpse into the future of the galaxy far, far away, only to discover that he had none.\", \"\\\"I don't know anything about it,\\\" Lucas said. \\\"I haven't seen it yet.\\\"\", \"Asked why, he explained that it was \\\"Because it's not in the movie theater. I like going to the movies and watching the whole thing there.\", \"\\\"I plan to see it when it's released.\\\"\", \"The filmmaker sold Lucasfilm and its attending franchises to Disney in October 2012 in a deal worth more than $4 billion. At the announcement, Disney revealed that Star Wars: Episode VII was in production, based on Lucas' outline. According to Disney's announcement, Lucas would serve as a \\\"creative consultant\\\" for the franchise.\", \"Disney announced the cast of the next installment, which includes actors from the original trilogy, in April 2014. The first trailer hit on Black Friday. Directed by J.J. Abrams, Star Wars: The Force Awakens is slated to open Dec. 15, 2015. You can watch the teaser trailer below. For more on what it might mean, be sure to read Polygon's analysis.\"], \"targetTitle\": \"George Lucas doesn't 'know anything about' the new Star Wars trailer\", \"targetDescription\": \"On YouTube alone, the teaser trailer for Star Wars: The Force Awakens has more than 12 million views, but you can count one person out of that number: George Lucas, the man who created the Star...\", \"targetUrl\": \"http://polygon.com/e/7119322\", \"provenance\": {\"source\": \"anonymized\", \"humanSpoiler\": \"George Lucas.\", \"spoilerPublisher\": \"SavedYouAClick\"}, \"spoiler\": [\"George Lucas\"], \"spoilerPositions\": [[[0, 151], [0, 163]]], \"tags\": [\"phrase\"]},\n",
    "    {\"uuid\": \"09f9794e-134e-4e58-8ec2-8259ec40c136\", \"postText\": [\"Has \\\"Star Trek 3\\\" found its director?\"], \"targetParagraphs\": [\"Joe Cornish could replace J.J. Abrams as king of the \\\"Star Trek\\\" universe. That's the report from Deadline.com's Mike Fleming, who writes that Paramount \\\"is sweet\\\" on the idea of Cornish directing the franchise's next installment.\", \"This isn't the first time Cornish, who directed the cult hit \\\"Attack the Block\\\" and co-wrote the script for \\\"Ant-Man\\\" with Edgar Wright, has had his name attached to \\\"Star Trek 3.\\\" Back in May, Latino Review reporter Umberto \\\"El Mayimbe\\\" Gonzalez tweeted that Cornish was under consideration as a possible replacement for Abrams, who is next directing \\\"Star Wars: Episode VII.\\\"\", \"I guess y'all wanna know about who might be directing STAR TREK 3 if it ever goes. Heard Joe Cornish BUT also heard he's on a list of names. — elmayimbe (@elmayimbe) May 23, 2013\", \"I'm NOT saying Joe Cornish is the guy, but what I am saying is the he is definitely one of NUMEROUS contenders. — elmayimbe (@elmayimbe) May 23, 2013\", \"Other reported contenders for \\\"Star Trek 3\\\" have included Jon M. Chu and Rupert Wyatt. In an email to HuffPost Entertainment, however, Chu's representatives denied that \\\"G.I. Joe: Retaliation\\\" director was up for the job. Wyatt's involvement was never confirmed or denied, but Abrams did discuss the \\\"Rise of the Planet of the Apes\\\" director in an interview with HitFix.\", \"\\\"Whomever it is that directs the film will be someone we all know is going to keep the cast and crew in good hands,\\\" Abrams told Collider back in September. \\\"I feel very lucky to have been part of it, and it definitely feels like the right time to let someone come in and do their own thing. I certainly don’t want someone to come in and try to do what I would have done. We want to hire someone who's gonna come in and bring their own sensibility. I'm very excited to see what comes next, despite feeling jealous of whomever that person is.\\\"\", \"HuffPost Entertainment contacted Cornish's representatives for comment on the Deadline.com rumor; this post will be updated if they respond. For more on Cornish, meanwhile, head to Deadline.com.\", \"[via Deadline.com]\"], \"targetTitle\": \"Joe Cornish Rumored For 'Star Trek 3' Director Job\", \"targetDescription\": \"Joe Cornish could replace J.J. Abrams as king of the \\\"Star Trek\\\" universe. That's the report from Deadline.com's Mike Fleming,\", \"targetUrl\": \"http://huff.to/1aVGhr4\", \"provenance\": {\"source\": \"anonymized\", \"humanSpoiler\": \"This article doesn't know but the rumor is Joe Cornish\", \"spoilerPublisher\": \"HuffPoSpoilers\"}, \"spoiler\": [\"Joe Cornish could replace J.J. Abrams as king of the \\\"Star Trek\\\" universe.\"], \"spoilerPositions\": [[[0, 0], [0, 74]]], \"tags\": [\"passage\"]},\n",
    "])\n",
    "\n",
    "# Create sentence and label lists\n",
    "sentences = df.postText.values\n",
    "\n",
    "# We need to add special tokens at the beginning and end of each sentence for XLNet to work properly\n",
    "sentences = [sentence[0] + \" [SEP] [CLS]\" for sentence in sentences]\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "\n",
    "MAX_LEN = 128\n",
    "# Use the XLNet tokenizer to convert the tokens to their index numbers in the XLNet vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask) \n",
    "\n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "  \n",
    "batch_size = 32  \n",
    "\n",
    "\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
    "# Prediction on test set\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask = batch\n",
    "  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
    "  with torch.no_grad():\n",
    "    # Forward pass, calculate logit predictions\n",
    "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    logits = outputs[0]\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)\n",
    "  decoding_array = {  \n",
    "        1: \"passage\",\n",
    "        2 : \"phrase\",\n",
    "        0 : \"multi\"\n",
    "        }\n",
    "  for z in predictions[0]:\n",
    "    label = np.where(z == z.max())[0][0]\n",
    "    decoded_label = decoding_array[label]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('anlp-Bd8LEaH2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "941b07e373631be7fd8257c1dcac349d04aaa134546f10f2da895e506c065c0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
