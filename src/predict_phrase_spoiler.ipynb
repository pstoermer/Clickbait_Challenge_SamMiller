{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from utils import load_data, improve_question, postprocess_phrase_spoiler, calculate_bleu\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import QuestionAnsweringPipeline, AutoTokenizer, AutoModelForQuestionAnswering"
   ],
   "metadata": {
    "id": "rEGd-uHRLiAq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load and Preprocess Data\n"
   ],
   "metadata": {
    "id": "HJxtmatCLmDn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train = load_data('../data/train.jsonl')\n",
    "val = load_data('../data/validation.jsonl')"
   ],
   "metadata": {
    "id": "6FKyzOeNLnut"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train['targetParagraphs'] = train.targetParagraphs.apply(lambda x: ' '.join(x))\n",
    "train['postText'] = train.postText.apply(lambda x: x[0].strip())\n",
    "train['spoiler'] = train.spoiler.apply(lambda x: '\\n'.join(x))\n",
    "train['tags'] = train.tags.apply(lambda x: x[0])\n",
    "train = train[train.tags == 'phrase'][['spoiler', 'postText', 'targetParagraphs', 'tags', 'spoilerPositions']]\n",
    "train['spoilerPositions'] = train.spoilerPositions.apply(lambda x: [x[0][0][1], x[0][1][1]])\n",
    "print(train.shape)\n",
    "train.head(3)"
   ],
   "metadata": {
    "id": "s9OEPFgnLofz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "val['targetParagraphs'] = val.targetParagraphs.apply(lambda x: ' '.join(x))\n",
    "val['postText'] = val.postText.apply(lambda x: x[0].strip())\n",
    "val['spoiler'] = val.spoiler.apply(lambda x: '\\n'.join(x))\n",
    "val['tags'] = val.tags.apply(lambda x: x[0])\n",
    "val = val[val.tags == 'phrase'][['spoiler', 'postText', 'targetParagraphs', 'tags', 'spoilerPositions']]\n",
    "val['spoilerPositions'] = val.spoilerPositions.apply(lambda x: [x[0][0][1], x[0][1][1]])\n",
    "print(val.shape)\n",
    "val.head(3)"
   ],
   "metadata": {
    "id": "gD0Qj3MtLsDG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_answers = pd.DataFrame()\n",
    "train_answers['text'] = train['spoiler']\n",
    "train_answers['answer_start'] = train.spoilerPositions.apply(lambda x: x[0])\n",
    "train_answers['answer_end'] = train.spoilerPositions.apply(lambda x: x[1])\n",
    "train_answers = train_answers.to_dict(orient='records')"
   ],
   "metadata": {
    "id": "JAEvu5ypLvXV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "val_answers = pd.DataFrame()\n",
    "val_answers['text'] = val['spoiler']\n",
    "val_answers['answer_start'] = val.spoilerPositions.apply(lambda x: x[0])\n",
    "val_answers['answer_end'] = val.spoilerPositions.apply(lambda x: x[1])\n",
    "val_answers = val_answers.to_dict(orient='records')"
   ],
   "metadata": {
    "id": "iZzDC80ALwTH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_contexts = train.targetParagraphs.to_list()\n",
    "train_questions = train.postText.to_list()\n",
    "val_contexts = val.targetParagraphs.to_list()\n",
    "val_questions = val.postText.to_list()"
   ],
   "metadata": {
    "id": "5sOKhmOwLyRQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Improve questions"
   ],
   "metadata": {
    "id": "_HN3_hmpL0An"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "val_questions[0]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "-inHL87AL_0A",
    "outputId": "746f9fc2-8371-4440-c203-a368f3cada88"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "improve_question(val_questions[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "improved_val_questions = [improve_question(q) for q in val_questions]"
   ],
   "metadata": {
    "id": "BYiZmMcbVzaP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Model and Tokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\", use_fast=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialize Question Answering Pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pipeline = QuestionAnsweringPipeline(model, tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get some examples for predictions with and without preprocessing of clickbaits"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pipeline(val_questions[0], val_contexts[0], postprocess=postprocess_phrase_spoiler)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pipeline(improve_question(val_questions[0]), val_contexts[0], postprocess=postprocess_phrase_spoiler)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "pred_answers = [\n",
    "    pipeline(q, c, postprocess=postprocess_phrase_spoiler)\n",
    "    for q, c in tqdm(zip(improved_val_questions, val_contexts))\n",
    "]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S-Dy1l-VV-nH",
    "outputId": "1e109769-a913-4674-f107-2d31c62564a2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate using BLEU Score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "val_answers_text = [a['text'] for a in val_answers]\n",
    "pred_answers_text = [a['answer'] for a in pred_answers]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "calculate_bleu(val_answers_text, pred_answers_text)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
